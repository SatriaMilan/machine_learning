# -*- coding: utf-8 -*-
"""Untitled45.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wSdCoVVa9d6fOhgK6pcGAH0_ftqX1kOW

# **JUNKCASH**
"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import matplotlib.pyplot as plt
import os
import shutil
from shutil import copyfile
import random
from PIL import Image

# Define a Callback class that stops training once accuracy reaches 99.9%
class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy') > 0.86 and logs.get('val_accuracy') > 0.86):
      print("\nReached 86% validation accuracy so cancelling training!")
      self.model.stop_training = True

"""**DATA**"""

#import data from google drive
from google.colab import drive
drive.mount('/content/drive')

source = '/content/drive/MyDrive/datasets'
output = '/content/drive/MyDrive/dataset/trash/processed_images'

list=[]

#total gambar setiap jenis gambar
for i in os.listdir(source):
    source_path_item = os.path.join(source, i)
    g = 0
    #file to jpg
    for j in os.listdir(source_path_item):
        if j[-3:] == "jpg" or j[-3:] == "gif":
            im = Image.open(os.path.join(source_path_item, j)).convert("RGB")
            im.save(os.path.join(source_path_item, j[:-3]+'jpeg'))

            #Removing file original
            os.remove(os.path.join(source_path_item, j))
            g += 1

    print("Convert Successful")
    print("Converted image =",g)
    print(f"There are {len(os.listdir(source_path_item))} images of", i + ".\n")
    list.append(i)

#split train and validating data

temp = '/content/drive/MyDrive/Temp'

if os.path.exists(temp):
    shutil.rmtree(temp)

#Making Split Train and Validation Folder
for i in list:
    os.makedirs(os.path.join(temp,"train",i))
    os.makedirs(os.path.join(temp,"validation",i))

def split_data(SOURCE_DIR, TRAINING_DIR, VALIDATION_DIR, SPLIT_SIZE):

  random_sample=random.sample(os.listdir(SOURCE_DIR),len(os.listdir(SOURCE_DIR)))

  #Split Data
  size=int(len(random_sample)*SPLIT_SIZE)

  target=TRAINING_DIR
  i=0

  for item in random_sample:

    item_source = os.path.join(SOURCE_DIR, item)

    if os.path.getsize(item_source) == 0:
      print(f'{item} jika 0,abaikan.')
    else:
      copyfile(item_source, os.path.join(target, item))
      i += 1

    if i == size:
      target = VALIDATION_DIR

train_dir = os.path.join(temp,"train")
val_dir = os.path.join(temp,"validation")

#Empty directories for run this cell multiple times
for i in os.listdir(train_dir):
  path = os.path.join(train_dir,i)
  if len(os.listdir(path))>0:
    for file in os.scandir(path):
      os.remove(file.path)

for i in os.listdir(val_dir):
  path = os.path.join(val_dir,i)
  if len(os.listdir(path))>0:
    for file in os.scandir(path):
      os.remove(file.path)

#Split Size
split_size = .8

#Calling Split Function
for i in list:
  split_data(os.path.join(source,i), os.path.join(train_dir, i), os.path.join(val_dir, i), split_size)
  print("Split", i, "Successful")
  print(f"There are {len(os.listdir(os.path.join(train_dir,i)))} images of", i + " for training.")
  print(f"There are {len(os.listdir(os.path.join(val_dir,i)))} images of", i + " for validation.\n")

"""**MODEL**"""

NUM_CLASSES = 4

#change for better accuracy based on your dataset
BATCH_SIZE = 64

base_model = tf.keras.applications.VGG16(input_shape=(224,224,3),
                                         include_top=False,
                                         weights='imagenet')

from tensorflow.keras import models, layers

EPOCHS = 15
for layer in base_model.layers:
 layer.trainable = False
x = tf.keras.layers.Flatten()(base_model.output)
x = tf.keras.layers.Dense(128, activation = 'relu')(x)
x = tf.keras.layers.Dense(64, activation = 'relu')(x)
x = tf.keras.layers.Dropout(0.2)(x)
x = tf.keras.layers.Dense(NUM_CLASSES, activation = 'softmax')(x)
model = tf.keras.models.Model(inputs = base_model.input, outputs = x)
model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(0.0001), metrics=['accuracy'])

model.summary()

train_datagen = ImageDataGenerator(rescale=1.0/255,
                                     rotation_range=20,
                                     width_shift_range=0.2,
                                     height_shift_range=0.2,
                                     horizontal_flip=True,
                                     fill_mode='nearest')

train_generator = train_datagen.flow_from_directory(directory=train_dir,target_size=(224,224), shuffle=False, class_mode='categorical')

validation_datagen = ImageDataGenerator(rescale=1.0/255,
                                     rotation_range=20,
                                     width_shift_range=0.2,
                                     height_shift_range=0.2,
                                     horizontal_flip=True,
                                     fill_mode='nearest')

validation_generator = validation_datagen.flow_from_directory(directory=val_dir, target_size=(224, 224), shuffle=False, class_mode='categorical')

callnew = myCallback()

training_steps_per_epoch = np.ceil(train_generator.samples / BATCH_SIZE)

validation_steps_per_epoch = np.ceil(validation_generator.samples / BATCH_SIZE)
# Compile the model
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Train the model
history = model.fit(train_generator, epochs=EPOCHS, validation_data=validation_generator)
print('Training Completed!')

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs=range(len(acc))
plt.plot(epochs, acc, 'r', label='Training accuracy')
plt.plot(epochs, val_acc, 'b', label='Validation accuracy')
plt.title('Training and validation accuracy')
plt.legend()
plt.figure()

plt.plot(epochs, loss, 'r', label='Training Loss')
plt.plot(epochs, val_loss, 'b', label='Validation Loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()

from mlxtend.plotting import plot_confusion_matrix
from sklearn.metrics import confusion_matrix, classification_report

Y_pred = model.predict_generator(validation_generator)
y_pred = np.argmax(Y_pred, axis=1)
mat = confusion_matrix(validation_generator.classes, y_pred)
print(mat)
plot_confusion_matrix(conf_mat=mat, figsize=(10,10), class_names= list)
print(classification_report(validation_generator.classes, y_pred, target_names=list))

keras_file = 'Model_1.h5'
tf.keras.models.save_model(model, keras_file)